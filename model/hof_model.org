#+TITLE: Hall of Fame Model
#+STARTUP: headlines
#+STARTUP: nohideblocks
#+STARTUP: noindent
#+OPTIONS: toc:4 h:4 ^:nil _:nil
#+PROPERTY: header-args:emacs-lisp :comments link
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+EXPORT_FILE_NAME: hall_of_fame_model.html
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>

* Activating Python Environment

  This code block must be run to activate python virtual environment for
  org session "SESSION_1". The following Python code blocks are run in
  "SESSION_1" in which the virtual environment should have been activated.

#+BEGIN_SRC emacs-lisp :session SESSION_1 :exports both
  (pyvenv-activate "~/gitRepos/TTK28-project/venv/")
#+END_SRC

#+RESULTS:

* Importing dependencies
  This project is primarily uses ~Keras~ and ~Sklearn~ for the data analysis
  itself. For data storage and manipulation ~pandas~ and ~numpy~ are the primary
  libraries used. The graphics are generated via ~matplotlib~.

#+begin_src python :session SESSION_1 :results output :exports both
  from datetime import datetime
  import pandas as pd
  import numpy as np
  import matplotlib
  matplotlib.use('PS')
  import matplotlib.pyplot as plt
  import logging
  from sklearn.preprocessing import LabelEncoder
  from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, auc
  from sklearn.utils import class_weight
  from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate, learning_curve
  import scikitplot as skplt
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import Model
  from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint
  from tensorflow.keras.models import Sequential, model_from_json, load_model
  from tensorflow.keras.layers import Dense
  from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
#+end_src

#+RESULTS:

* Logging Setup
  It was quite hard to keep track of different runs in the project, and noticed
  I kept loosing track of previous runs and just wanted a log to save some of
  the metrics and information about the different runs.
** Simply adding whitespace to log file
   Nothing special is happening here, but just making sure there is whitespace
   between runs in the log file.

#+begin_src python :session SESSION_1 :results output :exports both
  with open("../result/master_log.txt", "a") as file:
      file.write("\n")
      file.write("\n")
      print(HP, file=file)
#+end_src

#+RESULTS:

** Defining CSVLogger and Tensorboard Settings
   CSVLogger is used to keep track of the different runs manually, in addition
   to setting up tensorboard.

#+begin_src python :session SESSION_1 :results output :exports both
  csv_logger = CSVLogger('../result/master_log.txt', append=True, separator=';')
  log_dir = "../logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)
#+end_src

#+RESULTS:
: 2021-07-05 00:32:40.155378: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
: 2021-07-05 00:32:40.155416: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
: 2021-07-05 00:32:40.155461: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
: 2021-07-05 00:32:40.155488: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
: 2021-07-05 00:32:40.155508: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.

** STDOUT Logging Settings
   These are the settings for logging information to the terminal and debugging
   the output.

#+begin_src python :session SESSION_1 :results output :exports both
  logging.basicConfig(encoding='utf-8', level=logging.INFO)
#+end_src

#+RESULTS:

* Hyperparameters
  In order to more easily configure the model and keep track of the settting,
  this hyperparameters dictionary is defined here and used throughout the code
  where needed.

#+begin_src python :session SESSION_1 :results output :exports both
  HP = {
      'NAME': 'full',
      'INFO': 'checking_testing_final',
      'EPOCHS': 50,
      'FOLDS': 2,
      'BATCH_SIZE': 1,
      'OPTIMIZER': 'adam',
      'LOSS': 'binary_crossentropy',
      'METRICS': ['accuracy', 'Recall'],
      'DATASET': 'raw'
  }
#+end_src

#+RESULTS:

* Defining Helper Functions
** Creating an ROC Plot
   This is a plot that takes in the "false positives" (~fper~) and "true
   positives" (~tper~) and the name of which to save the plot. It generates,
   shows, and saves an /Receiver Operator Characteristics/ plot which is used
   for binary classifications problems. A good explanation can be found [[https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5][here]].

#+begin_src python :session SESSION_1 :results output :exports both
  def plot_roc_curve(fper, tper, name):
      plt.plot(fper, tper, color='orange', label='ROC')
      plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
      plt.xlabel('False Positive Rate')
      plt.ylabel('True Positive Rate')
      plt.title('Receiver Operating Characteristic (ROC) Curve')
      plt.legend()
      file_name = '../result/ROC_curve_' + name + datetime.now().strftime("%Y%m%d-%H%M%S") + '.png'
      plt.savefig(file_name)
      plt.savefig("../result/ROC_curve_latest.png")
      plt.clf()
#+end_src

#+RESULTS:

** Saving and Loading Models
   Simply functions to save and load varieties of this hof_model.

#+begin_src python :session SESSION_1 :results output :exports both
  def save_model(model, name):
      weights_name = "model_" + name +".h5"
      model.model.save_weights(weights_name)
      print("Saved model to disk")

  def load_model(model, name):
      model_name = "model_" + name + ".h5"
      model.load_weights(model_name)
      model.compile(optimizer= HP['OPTIMIZER'], loss= HP['LOSS'], metrics=HP['METRICS'])
      print("Loaded model from disk")
      return model
#+end_src

#+RESULTS:

* Importing Data
  Importing the training and test datasets, in addition to defining the column
  types to be used of throughout the model.

#+begin_src python :session SESSION_1 :results output :exports both
  train_df = pd.read_csv('../data/train_data_' + HP['NAME'] + '.csv', index_col=False)
  test_df = pd.read_csv('../data/test_data_' + HP['NAME'] + '.csv', index_col=False)
  data_type_dict = {'numerical': [ 'G_all', 'finalGame', 'OPS', 'Years_Played',
                                   'Most Valuable Player', 'AS_games', 'Gold Glove',
                                   'Rookie of the Year', 'World Series MVP', 'Silver Slugger'],
                    'categorical': ['HoF']}
#+end_src

#+RESULTS:

  These steps remove the labels from the data sources on both the test and
  training data.
#+begin_src python :session SESSION_1 :results output :exports both
  ### Removing the answers for the input data
  train_X_raw = train_df.drop(columns=['HoF'])
  train_y_raw = train_df['HoF']
  test_X_raw = test_df.drop(columns=['HoF'])
  test_y_raw = test_df['HoF']

  ### Converting pandas arrays to numpy arrays
  train_X = train_X_raw.to_numpy()
  test_X = test_X_raw.to_numpy()

  ### Creating the label data for the train and test sets
  encoder = LabelEncoder()
  train_y = encoder.fit_transform(train_y_raw)
  test_y = encoder.fit_transform(test_y_raw)
#+end_src

#+RESULTS:

* Defining and Compiling the Model
** Class Weights
  One of the ways of dealing with an imbalanced data set is to weight the
  classes. This suggestion and others are very nicely explained in this [[https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28][post]].
  An error in the one class will have a much higher cost in the cost function.
  The following backpropagation algorithm will correct the weights based on the
  ratio between the class weights.
#+begin_src python :session SESSION_1 :results output :exports both
  ### Weighting the classes for bias datasets
  # class_weights = class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)
  class_weights = {0: 1.0, 1: 15.0}
  print("class weights: ", class_weights)
  print("value counts of Y in train_y: ", train_y.sum())
  print("value counts of N in train_y: ", len(train_y) - train_y.sum())
#+end_src

#+RESULTS:
: class weights:  {0: 1.0, 1: 15.0}
: value counts of Y in train_y:  181
: value counts of N in train_y:  2552

** Checkpointing Model Weights
   This for setting up the checkpoints while running the model. It is not really
   in use right now as it is not added in the ~model.fit~ callbacks.
#+begin_src python :session SESSION_1 :results output :exports both
  model_weights_name = HP['NAME'] + '_model.h5'
  checkpointer = ModelCheckpoint(model_weights_name, monitor='Recall', verbose=0)
#+end_src

#+RESULTS:

** Defining and Training the Classifier
   Some different model structures were tested for this model, but there were
   some '[[https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=There%20are%20many%20rule-of,size%20of%20the%20output%20layer.][rules of thumb]]' which were considered. For the number of hidden nodes
   in a layer:
   - The number of hidden neurons should be between the size of the input layer
     and the size of the output layer.
   - The number of hidden neurons should be 2/3 the size of the input layer,
     plus the size of the output layer.
   - The number of hidden neurons should be less than twice the size of the
     input layer.

   A summation of these rules were outlined as the following:
   1. number of hidden layers equals one
   2. the number of neurons in that layer is the mean of the neurons in the input and output layers.

#+begin_src python :session SESSION_1 :results output :exports both
  def create_model():
      model = Sequential([
          Dense(10, activation='relu', input_shape=(10,)),
          Dense(5, activation='relu'),
          Dense(1, activation='sigmoid'),
      ])

      model.compile(
          optimizer= HP['OPTIMIZER'],
          loss= HP['LOSS'],
          metrics=HP['METRICS'])
      return model

  model = KerasClassifier(build_fn=create_model, epochs=HP['EPOCHS'],
                              batch_size=HP['BATCH_SIZE'], verbose = 2, )
  model.fit(train_X, train_y, callbacks=[csv_logger, tensorboard_callback])
  # model.fit(train_X, train_y, class_weight=class_weight, callbacks=[csv_logger, tensorboard_callback])
#+end_src

#+RESULTS:
#+begin_example
Epoch 1/50
2021-07-05 00:32:44.468326: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-07-05 00:32:44.468370: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-07-05 00:32:44.468421: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-07-05 00:32:44.492640: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-07-05 00:32:44.492726: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.
2021-07-05 00:32:44.493221: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-07-05 00:32:44.494175: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-07-05 00:32:44.495626: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44
2021-07-05 00:32:44.496398: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.trace.json.gz
2021-07-05 00:32:44.497566: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44
2021-07-05 00:32:44.497742: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.memory_profile.json.gz
2021-07-05 00:32:44.498042: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44Dumped tool data for xplane.pb to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.xplane.pb
Dumped tool data for overview_page.pb to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.overview_page.pb
Dumped tool data for input_pipeline.pb to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to ../logs/scalars/20210705-003240/train/plugins/profile/2021_07_05_00_32_44/BigArch.kernel_stats.pb

2733/2733 - 2s - loss: 0.2017 - accuracy: 0.9367 - recall: 0.1492
Epoch 2/50
2733/2733 - 2s - loss: 0.1331 - accuracy: 0.9499 - recall: 0.3757
Epoch 3/50
2733/2733 - 2s - loss: 0.1257 - accuracy: 0.9491 - recall: 0.4088
Epoch 4/50
2733/2733 - 2s - loss: 0.1191 - accuracy: 0.9543 - recall: 0.4807
Epoch 5/50
2733/2733 - 2s - loss: 0.1158 - accuracy: 0.9528 - recall: 0.5083
Epoch 6/50
2733/2733 - 2s - loss: 0.1148 - accuracy: 0.9491 - recall: 0.4641
Epoch 7/50
2733/2733 - 2s - loss: 0.1125 - accuracy: 0.9528 - recall: 0.5193
Epoch 8/50
2733/2733 - 2s - loss: 0.1104 - accuracy: 0.9517 - recall: 0.5414
Epoch 9/50
2733/2733 - 2s - loss: 0.1091 - accuracy: 0.9546 - recall: 0.5470
Epoch 10/50
2733/2733 - 2s - loss: 0.1075 - accuracy: 0.9543 - recall: 0.5691
Epoch 11/50
2733/2733 - 2s - loss: 0.1059 - accuracy: 0.9539 - recall: 0.5414
Epoch 12/50
2733/2733 - 2s - loss: 0.1038 - accuracy: 0.9557 - recall: 0.5801
Epoch 13/50
2733/2733 - 2s - loss: 0.1025 - accuracy: 0.9546 - recall: 0.5525
Epoch 14/50
2733/2733 - 2s - loss: 0.0993 - accuracy: 0.9579 - recall: 0.5746
Epoch 15/50
2733/2733 - 2s - loss: 0.1013 - accuracy: 0.9557 - recall: 0.5856
Epoch 16/50
2733/2733 - 2s - loss: 0.0992 - accuracy: 0.9565 - recall: 0.5691
Epoch 17/50
2733/2733 - 2s - loss: 0.0980 - accuracy: 0.9576 - recall: 0.5746
Epoch 18/50
2733/2733 - 2s - loss: 0.0972 - accuracy: 0.9623 - recall: 0.5967
Epoch 19/50
2733/2733 - 2s - loss: 0.0955 - accuracy: 0.9583 - recall: 0.5691
Epoch 20/50
2733/2733 - 2s - loss: 0.0960 - accuracy: 0.9557 - recall: 0.5912
Epoch 21/50
2733/2733 - 2s - loss: 0.0955 - accuracy: 0.9594 - recall: 0.5801
Epoch 22/50
2733/2733 - 2s - loss: 0.0944 - accuracy: 0.9587 - recall: 0.5801
Epoch 23/50
2733/2733 - 2s - loss: 0.0920 - accuracy: 0.9576 - recall: 0.5856
Epoch 24/50
2733/2733 - 2s - loss: 0.0915 - accuracy: 0.9612 - recall: 0.5912
Epoch 25/50
2733/2733 - 2s - loss: 0.0907 - accuracy: 0.9616 - recall: 0.6133
Epoch 26/50
2733/2733 - 2s - loss: 0.0930 - accuracy: 0.9601 - recall: 0.5635
Epoch 27/50
2733/2733 - 2s - loss: 0.0920 - accuracy: 0.9612 - recall: 0.5912
Epoch 28/50
2733/2733 - 2s - loss: 0.0906 - accuracy: 0.9590 - recall: 0.5801
Epoch 29/50
2733/2733 - 2s - loss: 0.0899 - accuracy: 0.9645 - recall: 0.6133
Epoch 30/50
2733/2733 - 2s - loss: 0.0903 - accuracy: 0.9598 - recall: 0.6133
Epoch 31/50
2733/2733 - 2s - loss: 0.0893 - accuracy: 0.9587 - recall: 0.5746
Epoch 32/50
2733/2733 - 2s - loss: 0.0902 - accuracy: 0.9598 - recall: 0.5801
Epoch 33/50
2733/2733 - 2s - loss: 0.0879 - accuracy: 0.9590 - recall: 0.5801
Epoch 34/50
2733/2733 - 2s - loss: 0.0873 - accuracy: 0.9612 - recall: 0.5967
Epoch 35/50
2733/2733 - 2s - loss: 0.0884 - accuracy: 0.9612 - recall: 0.5967
Epoch 36/50
2733/2733 - 2s - loss: 0.0848 - accuracy: 0.9638 - recall: 0.6298
Epoch 37/50
2733/2733 - 2s - loss: 0.0873 - accuracy: 0.9634 - recall: 0.6188
Epoch 38/50
2733/2733 - 2s - loss: 0.0868 - accuracy: 0.9638 - recall: 0.5967
Epoch 39/50
2733/2733 - 2s - loss: 0.0869 - accuracy: 0.9616 - recall: 0.5912
Epoch 40/50
2733/2733 - 2s - loss: 0.0848 - accuracy: 0.9649 - recall: 0.5967
Epoch 41/50
2733/2733 - 2s - loss: 0.0858 - accuracy: 0.9619 - recall: 0.5912
Epoch 42/50
2733/2733 - 2s - loss: 0.0861 - accuracy: 0.9619 - recall: 0.5912
Epoch 43/50
2733/2733 - 2s - loss: 0.0860 - accuracy: 0.9641 - recall: 0.6243
Epoch 44/50
2733/2733 - 2s - loss: 0.0850 - accuracy: 0.9634 - recall: 0.6022
Epoch 45/50
2733/2733 - 2s - loss: 0.0867 - accuracy: 0.9627 - recall: 0.6022
Epoch 46/50
2733/2733 - 2s - loss: 0.0842 - accuracy: 0.9638 - recall: 0.5912
Epoch 47/50
2733/2733 - 2s - loss: 0.0845 - accuracy: 0.9623 - recall: 0.6077
Epoch 48/50
2733/2733 - 2s - loss: 0.0856 - accuracy: 0.9630 - recall: 0.6022
Epoch 49/50
2733/2733 - 2s - loss: 0.0835 - accuracy: 0.9641 - recall: 0.5912
Epoch 50/50
2733/2733 - 2s - loss: 0.0843 - accuracy: 0.9605 - recall: 0.5801
#+end_example

* Saving or Loading the Classifier
  Both of these blocks can be uncommented to save or load models as desired.
** Saving the Model
#+begin_src python :session SESSION_1 :results output :exports both
  ### Saving Entire Model
  # save_model(model, "test_final_check")
  # model.model.save("model_test_final_different.h5")
#+end_src

#+RESULTS:

** Loading the Model
#+begin_src python :session SESSION_1 :results output :exports both
  ### Loading Entire Model
  # model = load_model(model, "test_final")
  # model.load("model_test_final_different.h5")
#+end_src

#+RESULTS:

* Evaluating and Metrics
** Predictions and Prediction Probabilities
  The ~model.predict()~ will return the class predictions for the input data put
  in the function. The ~model.predict_proba()~ will return the probability
  predictions for the classes, and is the likelihood of the observation
  belonging to the different classes.

#+begin_src python :session SESSION_1 :results output :exports both
  # Testing the model
  pred = model.predict(test_X)
  y_score = model.predict_proba(test_X, batch_size=HP['BATCH_SIZE'])
#+end_src

#+RESULTS:
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
:   warnings.warn('`model.predict_classes()` is deprecated and '
: 683/683 - 0s
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.
:   warnings.warn('`model.predict_proba()` is deprecated and '
: 683/683 - 0s

** Metrics
   Since the data set is imbalanced, there are other metrics to consider beyond
   the typical accuracy. In this dataset the ratio of HoF players vs. non-HoF
   players is ~14:1~ after preprocessing. Without class weights there could be a
   bias towards never selecting the HoF players resulting an accuracy over 90%
   while always predicting them as non-HoF players.
#+begin_src python :session SESSION_1 :results output :exports both
  # Calculating overall metrics
  accuracy = accuracy_score(test_y, pred)
  tn, fp, fn, tp = confusion_matrix(test_y, pred).ravel()
  confusion_mat = [tn, fp, fn, tp]
  auroc = roc_auc_score(test_y, y_score[:,0])
  precision = tp/(tp+fp)
  recall = tp/(tp+fn)
  f1 = (2*precision*recall)/(precision+recall)

  # Showing numerical results
  confusion_label = ["tn", "fp", "fn", "tp"]
  for i in range(0,len(confusion_mat)):
      print(confusion_label[i], ': ', confusion_mat[i])
  print("###### ---------Overall Results --------- ######")
  print("accuracy: ", accuracy)
  print("confusion_mat: ", confusion_mat)
  print("auroc: ", auroc)
  print("precision: ", precision)
  print("recall: ", recall)
  print("f1: ", f1)

  # Saving the metrics
  metric_dict = {
      'True Negative': tn,
      'True Positive': tp,
      'False Negative': fn,
      'False Positive': fp,
      'AUROC': auroc,
      'Accuracy': accuracy,
      'Precision': precision,
      'Recall': recall,
      'F1': f1
  }
  with open("../result/master_log.txt", "a") as file:
      print(metric_dict, file=file)
#+end_src

#+RESULTS:
#+begin_example
tn :  631
fp :  7
fn :  26
tp :  19
###### ---------Overall Results --------- ######
accuracy:  0.9516837481698389
confusion_mat:  [631, 7, 26, 19]
auroc:  0.041692789968652035
precision:  0.7307692307692307
recall:  0.4222222222222222
f1:  0.5352112676056338
#+end_example

* Plots
  Three metrics that are smart to include for imbalanced datasets are:
  - ROC curve
  - Confusion Matrix
  - Precision-Recall Matrix

#+begin_src python :session SESSION_1 :results output :exports both
  # ROC curve
  fper, tper, thresholds = roc_curve(test_y, y_score[:,1])

  plot_roc_curve(fper, tper, HP['NAME'])
  man_auroc = auc(fper, tper)
  print("man_auroc: ", man_auroc)
#+end_src

#+RESULTS:
: Trying to create RO
: man_auroc:  0.9583072100313479


The latest ROC_plot is the following plot.

[[file:ROC_curve_latest.png]]


#+begin_src python :session SESSION_1 :results output :exports both
# Generating a confusion matrix
skplt.metrics.plot_confusion_matrix(test_y, pred, normalize=True)
confusion_mat_string = "../result/confusion_mat_" + HP['NAME']+ datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(confusion_mat_string)
plt.savefig("../result/confusion_mat_latest.png")
plt.clf()
#+end_src

#+RESULTS:


The confusion matrix for the last run.

[[file:confusion_mat_latest.png]]


#+begin_src python :session SESSION_1 :results output :exports both
# Generating precision-recall curve
skplt.metrics.plot_precision_recall(test_y, y_score)
precision_recall_curve_string = "../result/precision_recall_curve_" + HP['NAME'] + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(precision_recall_curve_string)
plt.savefig("../result/precision_recall_curve_latest.png")
plt.clf()
#+end_src

#+RESULTS:


[[file:precision_recall_curve_latest.png]]


* Links to Other Files in Project

  1. [[https://www.olavpedersen.com/standalone_hof/extract_data.html][extract_data]]: Extracting data from Lahman's raw data.
  2. [[https://www.olavpedersen.com/standalone_hof/filter_data.html][filter_data]]: Data manipulation, feature creation and feature selection.
  3. [[https://www.olavpedersen.com/standalone_hof/hall_of_fame_model.html][hof_model]]: Creation of the model and training of the neural network.

* Local Variables                                          :noexport:ARCHIVE:
# Local Variables:
# after-save-hook: org-html-export-to-html
# End:
