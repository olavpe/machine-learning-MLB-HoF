#+TITLE: Hall of Fame Model
#+STARTUP: headlines
#+STARTUP: nohideblocks
#+STARTUP: noindent
#+OPTIONS: toc:4 h:4 ^:nil _:nil
#+PROPERTY: header-args:emacs-lisp :comments link
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+EXPORT_FILE_NAME: hall_of_fame_model.html
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>

* Activating Python Environment

  This code block must be run to activate python virtual environment for
  org session "SESSION_1". The following Python code blocks are run in
  "SESSION_1" in which the virtual environment should have been activated.

#+BEGIN_SRC emacs-lisp :session SESSION_1 :exports both
  (pyvenv-activate "~/gitRepos/TTK28-project/venv/")
#+END_SRC

#+RESULTS:

* Importing dependencies
  This project is primarily uses ~Keras~ and ~Sklearn~ for the data analysis
  itself. For data storage and manipulation ~pandas~ and ~numpy~ are the primary
  libraries used. The graphics are generated via ~matplotlib~.
  
#+begin_src python :session SESSION_1 :results output :exports both
  from datetime import datetime
  import pandas as pd
  import numpy as np
  # import matplotlib
  # matplotlib.use('Qt5Agg')
  import matplotlib.pyplot as plt
  import logging
  from sklearn.preprocessing import LabelEncoder
  from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, auc
  from sklearn.utils import class_weight
  from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate, learning_curve
  import scikitplot as skplt
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import Model
  from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint
  from tensorflow.keras.models import Sequential, model_from_json, load_model
  from tensorflow.keras.layers import Dense
  from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
#+end_src

#+RESULTS:
: 2021-07-01 12:13:35.128730: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
: 2021-07-01 12:13:35.128762: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

* Logging Setup
  It was quite hard to keep track of different runs in the project, and noticed
  I kept loosing track of previous runs and just wanted a log to save some of
  the metrics and information about the different runs.
** Simply adding whitespace to log file 
   Nothing special is happening here, but just making sure there is whitespace
   between runs in the log file.
   
#+begin_src python :session SESSION_1 :results output :exports both
  with open("../result/master_log.txt", "a") as file:
      file.write("\n")
      file.write("\n")
      print(HP, file=file)
#+end_src

#+RESULTS:

** Defining CSVLogger and Tensorboard Settings
   CSVLogger is used to keep track of the different runs manually, in addition
   to setting up tensorboard.
   
#+begin_src python :session SESSION_1 :results output :exports both
  csv_logger = CSVLogger('../result/master_log.txt', append=True, separator=';')
  log_dir = "../logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)
#+end_src

#+RESULTS:
: 2021-07-01 12:13:43.534150: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
: 2021-07-01 12:13:43.534220: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
: 2021-07-01 12:13:43.564739: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
: 2021-07-01 12:13:43.587808: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.

** STDOUT Logging Settings
   These are the settings for logging information to the terminal and debugging
   the output.
   
#+begin_src python :session SESSION_1 :results output :exports both
  logging.basicConfig(encoding='utf-8', level=logging.INFO)
#+end_src

#+RESULTS:
   
* Hyperparameters
  In order to more easily configure the model and keep track of the settting,
  this hyperparameters dictionary is defined here and used throughout the code
  where needed. 

#+begin_src python :session SESSION_1 :results output :exports both
  HP = {
      'NAME': 'full',
      'INFO': 'checking_testing_final',
      'EPOCHS': 50,
      'FOLDS': 2,
      'BATCH_SIZE': 1,
      'OPTIMIZER': 'adam',
      'LOSS': 'binary_crossentropy',
      'METRICS': ['accuracy', 'Recall'],
      'DATASET': 'raw'
  }
#+end_src

#+RESULTS:

* Defining Helper Functions 
** Creating an ROC Plot   
   This is a plot that takes in the "false positives" (~fper~) and "true
   positives" (~tper~) and the name of which to save the plot. It generates,
   shows, and saves an /Receiver Operator Characteristics/ plot which is used
   for binary classifications problems. A good explanation can be found [[https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5][here]].

#+begin_src python :session SESSION_1 :results output :exports both
  def plot_roc_curve(fper, tper, name):
      plt.plot(fper, tper, color='orange', label='ROC')
      plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
      plt.xlabel('False Positive Rate')
      plt.ylabel('True Positive Rate')
      plt.title('Receiver Operating Characteristic (ROC) Curve')
      plt.legend()
      file_name = '../result/ROC_curve_' + name + datetime.now().strftime("%Y%m%d-%H%M%S") + '.png'
      plt.savefig(file_name)
      plt.savefig("../result/ROC_curve_latest.png")
      plt.clf()
#+end_src

#+RESULTS:

** Saving and Loading Models   
   Simply functions to save and load varieties of this hof_model.
   
#+begin_src python :session SESSION_1 :results output :exports both
  def save_model(model, name):
      weights_name = "model_" + name +".h5"
      model.model.save_weights(weights_name)
      print("Saved model to disk")

  def load_model(model, name):
      model_name = "model_" + name + ".h5"
      model.load_weights(model_name)
      model.compile(optimizer= HP['OPTIMIZER'], loss= HP['LOSS'], metrics=HP['METRICS'])
      print("Loaded model from disk")
      return model
#+end_src

#+RESULTS:

* Importing Data 
  Importing the training and test datasets, in addition to defining the column
  types to be used of throughout the model.
  
#+begin_src python :session SESSION_1 :results output :exports both
  train_df = pd.read_csv('../data/train_data_' + HP['NAME'] + '.csv', index_col=False)
  test_df = pd.read_csv('../data/test_data_' + HP['NAME'] + '.csv', index_col=False)
  data_type_dict = {'numerical': [ 'G_all', 'finalGame', 'OPS', 'Years_Played',
                                   'Most Valuable Player', 'AS_games', 'Gold Glove',
                                   'Rookie of the Year', 'World Series MVP', 'Silver Slugger'],
                    'categorical': ['HoF']}
#+end_src

#+RESULTS:

  These steps remove the labels from the data sources on both the test and
  training data. 
#+begin_src python :session SESSION_1 :results output :exports both
  ### Removing the answers for the input data
  train_X_raw = train_df.drop(columns=['HoF'])
  train_y_raw = train_df['HoF']
  test_X_raw = test_df.drop(columns=['HoF'])
  test_y_raw = test_df['HoF']

  ### Converting pandas arrays to numpy arrays
  train_X = train_X_raw.to_numpy()
  test_X = test_X_raw.to_numpy()

  ### Creating the label data for the train and test sets
  encoder = LabelEncoder()
  train_y = encoder.fit_transform(train_y_raw)
  test_y = encoder.fit_transform(test_y_raw)
#+end_src

#+RESULTS:

* Defining and Compiling the Model
** Class Weights 
  One of the ways of dealing with an imbalanced data set is to weight the
  classes. This suggestion and others are very nicely explained in this [[https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28][post]].
  An error in the one class will have a much higher cost in the cost function.
  The following backpropagation algorithm will correct the weights based on the
  ratio between the class weights.
#+begin_src python :session SESSION_1 :results output :exports both
  ### Weighting the classes for bias datasets
  # class_weights = class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)
  class_weights = {0: 1.0, 1: 15.0}
  print("class weights: ", class_weights)
  print("value counts of Y in train_y: ", train_y.sum())
  print("value counts of N in train_y: ", len(train_y) - train_y.sum())
#+end_src

#+RESULTS:
: class weights:  {0: 1.0, 1: 15.0}
: value counts of Y in train_y:  181
: value counts of N in train_y:  2552

** Checkpointing Model Weights
   This for setting up the checkpoints while running the model. It is not really
   in use right now as it is not added in the ~model.fit~ callbacks.
#+begin_src python :session SESSION_1 :results output :exports both
  model_weights_name = HP['NAME'] + '_model.h5'
  checkpointer = ModelCheckpoint(model_weights_name, monitor='Recall', verbose=0)
#+end_src

#+RESULTS:

** Defining and Training the Classifier
   Some different model structures were tested for this model, but there were
   some '[[https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=There%20are%20many%20rule-of,size%20of%20the%20output%20layer.][rules of thumb]]' which were considered. For the number of hidden nodes
   in a layer:
   - The number of hidden neurons should be between the size of the input layer
     and the size of the output layer. 
   - The number of hidden neurons should be 2/3 the size of the input layer,
     plus the size of the output layer. 
   - The number of hidden neurons should be less than twice the size of the
     input layer. 

   A summation of these rules were outlined as the following:
   1. number of hidden layers equals one
   2. the number of neurons in that layer is the mean of the neurons in the input and output layers.

#+begin_src python :session SESSION_1 :results output :exports both
  def create_model():
      model = Sequential([
          Dense(10, activation='relu', input_shape=(10,)),
          Dense(5, activation='relu'),
          Dense(1, activation='sigmoid'),
      ])

      model.compile(
          optimizer= HP['OPTIMIZER'],
          loss= HP['LOSS'],
          metrics=HP['METRICS'])
      return model

  model = KerasClassifier(build_fn=create_model, epochs=HP['EPOCHS'],
                              batch_size=HP['BATCH_SIZE'], verbose = 2, )
  model.fit(train_X, train_y, callbacks=[csv_logger, tensorboard_callback])
  # model.fit(train_X, train_y, class_weight=class_weight, callbacks=[csv_logger, tensorboard_callback])
#+end_src

#+RESULTS:
#+begin_example
2021-07-01 12:14:11.759961: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-07-01 12:14:11.774593: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-07-01 12:14:11.774676: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: BigArch
2021-07-01 12:14:11.774698: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: BigArch
2021-07-01 12:14:11.775821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.143.0
2021-07-01 12:14:11.775889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.143.0
2021-07-01 12:14:11.775911: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.143.0
2021-07-01 12:14:11.777678: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-07-01 12:14:11.923812: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-07-01 12:14:11.946133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2295530000 Hz
Epoch 1/50
2021-07-01 12:14:12.651743: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-07-01 12:14:12.651780: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-07-01 12:14:12.655400: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-07-01 12:14:12.661855: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-07-01 12:14:12.672498: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12
2021-07-01 12:14:12.673452: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.trace.json.gz
2021-07-01 12:14:12.682579: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12
2021-07-01 12:14:12.682964: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.memory_profile.json.gz
2021-07-01 12:14:12.683565: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12Dumped tool data for xplane.pb to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.xplane.pb
Dumped tool data for overview_page.pb to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.overview_page.pb
Dumped tool data for input_pipeline.pb to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to ../logs/scalars/20210701-121343/train/plugins/profile/2021_07_01_12_14_12/BigArch.kernel_stats.pb

2733/2733 - 2s - loss: 0.2993 - accuracy: 0.9100 - recall: 0.2155
Epoch 2/50
2733/2733 - 2s - loss: 0.1430 - accuracy: 0.9433 - recall: 0.2652
Epoch 3/50
2733/2733 - 2s - loss: 0.1261 - accuracy: 0.9491 - recall: 0.3646
Epoch 4/50
2733/2733 - 2s - loss: 0.1196 - accuracy: 0.9528 - recall: 0.5028
Epoch 5/50
2733/2733 - 2s - loss: 0.1163 - accuracy: 0.9535 - recall: 0.4917
Epoch 6/50
2733/2733 - 2s - loss: 0.1126 - accuracy: 0.9568 - recall: 0.5193
Epoch 7/50
2733/2733 - 2s - loss: 0.1130 - accuracy: 0.9561 - recall: 0.5746
Epoch 8/50
2733/2733 - 2s - loss: 0.1083 - accuracy: 0.9568 - recall: 0.5635
Epoch 9/50
2733/2733 - 2s - loss: 0.1082 - accuracy: 0.9557 - recall: 0.5525
Epoch 10/50
2733/2733 - 2s - loss: 0.1079 - accuracy: 0.9535 - recall: 0.5249
Epoch 11/50
2733/2733 - 2s - loss: 0.1068 - accuracy: 0.9576 - recall: 0.5635
Epoch 12/50
2733/2733 - 2s - loss: 0.1054 - accuracy: 0.9572 - recall: 0.5470
Epoch 13/50
2733/2733 - 2s - loss: 0.1044 - accuracy: 0.9576 - recall: 0.5967
Epoch 14/50
2733/2733 - 2s - loss: 0.1029 - accuracy: 0.9583 - recall: 0.5525
Epoch 15/50
2733/2733 - 2s - loss: 0.1018 - accuracy: 0.9598 - recall: 0.6077
Epoch 16/50
2733/2733 - 2s - loss: 0.1012 - accuracy: 0.9594 - recall: 0.5967
Epoch 17/50
2733/2733 - 2s - loss: 0.1000 - accuracy: 0.9605 - recall: 0.6133
Epoch 18/50
2733/2733 - 2s - loss: 0.1014 - accuracy: 0.9598 - recall: 0.6077
Epoch 19/50
2733/2733 - 2s - loss: 0.0984 - accuracy: 0.9583 - recall: 0.5746
Epoch 20/50
2733/2733 - 2s - loss: 0.0975 - accuracy: 0.9605 - recall: 0.6298
Epoch 21/50
2733/2733 - 2s - loss: 0.0985 - accuracy: 0.9605 - recall: 0.5746
Epoch 22/50
2733/2733 - 2s - loss: 0.0959 - accuracy: 0.9601 - recall: 0.5856
Epoch 23/50
2733/2733 - 2s - loss: 0.0963 - accuracy: 0.9619 - recall: 0.6133
Epoch 24/50
2733/2733 - 2s - loss: 0.0949 - accuracy: 0.9601 - recall: 0.5912
Epoch 25/50
2733/2733 - 2s - loss: 0.0944 - accuracy: 0.9619 - recall: 0.6243
Epoch 26/50
2733/2733 - 2s - loss: 0.0943 - accuracy: 0.9630 - recall: 0.6133
Epoch 27/50
2733/2733 - 2s - loss: 0.0926 - accuracy: 0.9623 - recall: 0.6243
Epoch 28/50
2733/2733 - 2s - loss: 0.0930 - accuracy: 0.9619 - recall: 0.6188
Epoch 29/50
2733/2733 - 2s - loss: 0.0913 - accuracy: 0.9594 - recall: 0.5967
Epoch 30/50
2733/2733 - 2s - loss: 0.0905 - accuracy: 0.9616 - recall: 0.6077
Epoch 31/50
2733/2733 - 2s - loss: 0.0902 - accuracy: 0.9612 - recall: 0.6243
Epoch 32/50
2733/2733 - 2s - loss: 0.0898 - accuracy: 0.9619 - recall: 0.6022
Epoch 33/50
2733/2733 - 2s - loss: 0.0870 - accuracy: 0.9638 - recall: 0.6243
Epoch 34/50
2733/2733 - 2s - loss: 0.0890 - accuracy: 0.9630 - recall: 0.6298
Epoch 35/50
2733/2733 - 2s - loss: 0.0871 - accuracy: 0.9645 - recall: 0.6464
Epoch 36/50
2733/2733 - 2s - loss: 0.0867 - accuracy: 0.9630 - recall: 0.6409
Epoch 37/50
2733/2733 - 2s - loss: 0.0855 - accuracy: 0.9634 - recall: 0.6354
Epoch 38/50
2733/2733 - 2s - loss: 0.0855 - accuracy: 0.9645 - recall: 0.6464
Epoch 39/50
2733/2733 - 2s - loss: 0.0840 - accuracy: 0.9671 - recall: 0.6685
Epoch 40/50
2733/2733 - 2s - loss: 0.0856 - accuracy: 0.9638 - recall: 0.6464
Epoch 41/50
2733/2733 - 2s - loss: 0.0838 - accuracy: 0.9641 - recall: 0.6188
Epoch 42/50
2733/2733 - 2s - loss: 0.0841 - accuracy: 0.9638 - recall: 0.6519
Epoch 43/50
2733/2733 - 2s - loss: 0.0841 - accuracy: 0.9619 - recall: 0.6409
Epoch 44/50
2733/2733 - 2s - loss: 0.0820 - accuracy: 0.9649 - recall: 0.6796
Epoch 45/50
2733/2733 - 2s - loss: 0.0813 - accuracy: 0.9652 - recall: 0.6519
Epoch 46/50
2733/2733 - 2s - loss: 0.0806 - accuracy: 0.9638 - recall: 0.6354
Epoch 47/50
2733/2733 - 2s - loss: 0.0820 - accuracy: 0.9660 - recall: 0.6796
Epoch 48/50
2733/2733 - 2s - loss: 0.0796 - accuracy: 0.9671 - recall: 0.6796
Epoch 49/50
2733/2733 - 2s - loss: 0.0817 - accuracy: 0.9649 - recall: 0.6464
Epoch 50/50
2733/2733 - 2s - loss: 0.0822 - accuracy: 0.9641 - recall: 0.6630
#+end_example

* Saving or Loading the Classifier
  Both of these blocks can be uncommented to save or load models as desired.
** Saving the Model 
#+begin_src python :session SESSION_1 :results output :exports both
  ### Saving Entire Model
  # save_model(model, "test_final_check")
  # model.model.save("model_test_final_different.h5")
#+end_src

#+RESULTS:

** Loading the Model 
#+begin_src python :session SESSION_1 :results output :exports both
  ### Loading Entire Model
  # model = load_model(model, "test_final")
  # model.load("model_test_final_different.h5")
#+end_src

#+RESULTS:

* Evaluating and Metrics
** Predictions and Prediction Probabilities
  The ~model.predict()~ will return the class predictions for the input data put
  in the function. The ~model.predict_proba()~ will return the probability
  predictions for the classes, and is the likelihood of the observation
  belonging to the different classes.
  
#+begin_src python :session SESSION_1 :results output :exports both
  # Testing the model
  pred = model.predict(test_X)
  y_score = model.predict_proba(test_X, batch_size=HP['BATCH_SIZE'])
#+end_src

#+RESULTS:
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
:   warnings.warn('`model.predict_classes()` is deprecated and '
: 683/683 - 0s
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.
:   warnings.warn('`model.predict_proba()` is deprecated and '
: 683/683 - 0s

** Metrics
   Since the data set is imbalanced, there are other metrics to consider beyond
   the typical accuracy. In this dataset the ratio of HoF players vs. non-HoF
   players is ~14:1~ after preprocessing. Without class weights there could be a
   bias towards never selecting the HoF players resulting an accuracy over 90%
   while always predicting them as non-HoF players.
#+begin_src python :session SESSION_1 :results output :exports both
  # Calculating overall metrics
  accuracy = accuracy_score(test_y, pred)
  tn, fp, fn, tp = confusion_matrix(test_y, pred).ravel()
  confusion_mat = [tn, fp, fn, tp]
  auroc = roc_auc_score(test_y, y_score[:,0])
  precision = tp/(tp+fp)
  recall = tp/(tp+fn)
  f1 = (2*precision*recall)/(precision+recall)

  # Showing numerical results
  confusion_label = ["tn", "fp", "fn", "tp"]
  for i in range(0,len(confusion_mat)):
      print(confusion_label[i], ': ', confusion_mat[i])
  print("###### ---------Overall Results --------- ######")
  print("accuracy: ", accuracy)
  print("confusion_mat: ", confusion_mat)
  print("auroc: ", auroc)
  print("precision: ", precision)
  print("recall: ", recall)
  print("f1: ", f1)

  # Saving the metrics
  metric_dict = {
      'True Negative': tn,
      'True Positive': tp,
      'False Negative': fn,
      'False Positive': fp,
      'AUROC': auroc,
      'Accuracy': accuracy,
      'Precision': precision,
      'Recall': recall,
      'F1': f1
  }
  with open("../result/master_log.txt", "a") as file:
      print(metric_dict, file=file)
#+end_src

#+RESULTS:
#+begin_example
tn :  631
fp :  7
fn :  24
tp :  21
###### ---------Overall Results --------- ######
accuracy:  0.9546120058565154
confusion_mat:  [631, 7, 24, 21]
auroc:  0.04869383490073146
precision:  0.75
recall:  0.4666666666666667
f1:  0.5753424657534245
#+end_example

* Plots
  Three metrics that are smart to include for imbalanced datasets are:
  - ROC curve
  - Confusion Matrix
  - Precision-Recall Matrix
    
#+begin_src python :session SESSION_1 :results output :exports both
  # ROC curve
  fper, tper, thresholds = roc_curve(test_y, y_score[:,1])
  plot_roc_curve(fper, tper, HP['NAME'])
  man_auroc = auc(fper, tper)
  print("man_auroc: ", man_auroc)
#+end_src

[[file:../result/ROC_curve_latest.png]]

#+RESULTS:
: man_auroc:  0.9676593521421109

#+begin_src python :session SESSION_1 :results output :exports both :noweb yes
# Generating a confusion matrix
skplt.metrics.plot_confusion_matrix(test_y, pred, normalize=True)
confusion_mat_string = "../result/confusion_mat_" + HP['NAME']+ datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(confusion_mat_string)
plt.savefig("../result/confusion_mat_latest.png")
plt.clf()
#+end_src

#+RESULTS:

[[file:../result/confusion_mat_latest.png]]


#+begin_src python :session SESSION_1 :results output :exports both :noweb yes
# Generating precision-recall curve
skplt.metrics.plot_precision_recall(test_y, y_score)
precision_recall_curve_string = "../result/precision_recall_curve_" + HP['NAME'] + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(precision_recall_curve_string)
plt.savefig("../result/precision_recall_curve_latest.png")
plt.clf()
#+end_src

[[file:../result/precision_recall_curve_latest.png]]



* Local Variables                                          :noexport:ARCHIVE:
# Local Variables:
# after-save-hook: org-html-export-to-html
# End:
