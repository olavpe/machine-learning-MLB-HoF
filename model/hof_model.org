#+TITLE: Hall of Fame Model
#+STARTUP: headlines
#+STARTUP: nohideblocks
#+STARTUP: noindent
#+OPTIONS: toc:4 h:4 ^:nil _:nil
#+PROPERTY: header-args:emacs-lisp :comments link
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+EXPORT_FILE_NAME: hall_of_fame_model.html
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>

* Activating Python Environment

  This code block must be run to activate python virtual environment for
  org session "SESSION_1". The following Python code blocks are run in
  "SESSION_1" in which the virtual environment should have been activated.

#+BEGIN_SRC emacs-lisp :session SESSION_1 :exports both
  (pyvenv-activate "~/gitRepos/TTK28-project/venv/")
#+END_SRC

#+RESULTS:

* Importing dependencies
  This project is primarily uses ~Keras~ and ~Sklearn~ for the data analysis
  itself. For data storage and manipulation ~pandas~ and ~numpy~ are the primary
  libraries used. The graphics are generated via ~matplotlib~.

#+begin_src python :session SESSION_1 :results output :exports both
  from datetime import datetime
  import pandas as pd
  import numpy as np
  import matplotlib
  matplotlib.use('PS')
  import matplotlib.pyplot as plt
  import logging
  from sklearn import tree, decomposition
  from sklearn.preprocessing import LabelEncoder
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, auc, matthews_corrcoef, make_scorer
  from sklearn.utils import class_weight
  from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate, learning_curve
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV
  import scikitplot as skplt
  import tensorflow as tf
  from tensorflow import keras
  from tensorflow.keras import Model
  from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint
  from tensorflow.keras.models import Sequential, model_from_json, load_model
  from tensorflow.keras.layers import Dense
  from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
#+end_src

#+RESULTS:

* Hyperparameters
  In order to more easily configure the model and keep track of the settting,
  this hyperparameters dictionary is defined here and used throughout the code
  where needed.

#+begin_src python :session SESSION_1 :results output :exports both
  HP = {
      'NAME': 'full',
      'INFO': 'checking_testing_final',
      'EPOCHS': 50,
      'FOLDS': 2,
      'BATCH_SIZE': 1,
      'OPTIMIZER': 'adam',
      'LOSS': 'binary_crossentropy',
      'METRICS': ['accuracy', 'Recall'],
      'DATASET': 'raw'
  }
#+end_src

#+RESULTS:

* Logging Setup
  It was quite hard to keep track of different runs in the project, and noticed
  I kept loosing track of previous runs and just wanted a log to save some of
  the metrics and information about the different runs.
** Simply adding whitespace to log file
   Nothing special is happening here, but just making sure there is whitespace
   between runs in the log file.

#+begin_src python :session SESSION_1 :results output :exports both
  with open("../result/master_log.txt", "a") as file:
      file.write("\n")
      file.write("\n")
      print(HP, file=file)
#+end_src

#+RESULTS:

** Defining CSVLogger and Tensorboard Settings
   CSVLogger is used to keep track of the different runs manually, in addition
   to setting up tensorboard.

#+begin_src python :session SESSION_1 :results output :exports both
  csv_logger = CSVLogger('../result/master_log.txt', append=True, separator=';')
  log_dir = "../logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)
#+end_src

#+RESULTS:
: 2021-07-25 00:06:32.404901: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
: 2021-07-25 00:06:32.404938: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
: 2021-07-25 00:06:32.404992: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
: 2021-07-25 00:06:32.405028: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
: 2021-07-25 00:06:32.405048: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.

** STDOUT Logging Settings
   These are the settings for logging information to the terminal and debugging
   the output.

#+begin_src python :session SESSION_1 :results output :exports both
  # logging.basicConfig(encoding='utf-8', level=logging.INFO)
#+end_src

#+RESULTS:

* Defining Helper Functions
** Creating an ROC Plot
   This is a plot that takes in the "false positives" (~fper~) and "true
   positives" (~tper~) and the name of which to save the plot. It generates,
   shows, and saves an /Receiver Operator Characteristics/ plot which is used
   for binary classifications problems. A good explanation can be found [[https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5][here]].

#+begin_src python :session SESSION_1 :results output :exports both
  def plot_roc_curve(fper, tper, name):
      plt.plot(fper, tper, color='orange', label='ROC')
      plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
      plt.xlabel('False Positive Rate')
      plt.ylabel('True Positive Rate')
      plt.title('Receiver Operating Characteristic (ROC) Curve')
      plt.legend()
      file_name = '../result/ROC_curve_' + name + datetime.now().strftime("%Y%m%d-%H%M%S") + '.png'
      plt.savefig(file_name)
      plt.savefig("../result/ROC_curve_latest.png")
#+end_src

#+RESULTS:

** Saving and Loading Models
   Simply functions to save and load varieties of this hof_model.

#+begin_src python :session SESSION_1 :results output :exports both
  def save_model(model, name):
      weights_name = "model_" + name +".h5"
      model.model.save_weights(weights_name)
      print("Saved model to disk")

  def load_model(model, name):
      model_name = "model_" + name + ".h5"
      model.load_weights(model_name)
      model.compile(optimizer= HP['OPTIMIZER'], loss= HP['LOSS'], metrics=HP['METRICS'])
      print("Loaded model from disk")
      return model
#+end_src

#+RESULTS:

* Importing Data
  Importing the training and test datasets, in addition to defining the column
  types to be used of throughout the model.

#+begin_src python :session SESSION_1 :results output :exports both
  train_df = pd.read_csv('../data/train_data_' + HP['NAME'] + '.csv', index_col=False)
  test_df = pd.read_csv('../data/test_data_' + HP['NAME'] + '.csv', index_col=False)
  data_type_dict = {'numerical': [ 'G_all', 'finalGame', 'OPS', 'SB', 'HR', 'Years_Played',
                                   'Most Valuable Player', 'AS_games', 'Gold Glove',
                                   'Rookie of the Year', 'World Series MVP', 'Silver Slugger',
                                   'WHIP', 'ERA', 'KperBB', 'PO', 'A', 'E', 'DP'],
                    'categorical': ['HoF']}
#+end_src

#+RESULTS:

  These steps remove the labels from the data sources on both the test and
  training data.
#+begin_src python :session SESSION_1 :results output :exports both
  ### Removing the answers for the input data
  train_X_raw = train_df.drop(columns=['HoF'])
  train_y_raw = train_df['HoF']
  test_X_raw = test_df.drop(columns=['HoF'])
  test_y_raw = test_df['HoF']

  ### Converting pandas arrays to numpy arrays
  train_X = train_X_raw.to_numpy()
  test_X = test_X_raw.to_numpy()

  ### Creating the label data for the train and test sets
  encoder = LabelEncoder()
  train_y = encoder.fit_transform(train_y_raw)
  test_y = encoder.fit_transform(test_y_raw)
#+end_src

#+RESULTS:

* Class Weights
  One of the ways of dealing with an imbalanced data set is to weight the
  classes. This suggestion and others are very nicely explained in this [[https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28][post]].
  An error in the one class will have a much higher cost in the cost function.
  The following backpropagation algorithm will correct the weights based on the
  ratio between the class weights.
#+begin_src python :session SESSION_1 :results output :exports both
  class_weights = class_weight.compute_class_weight('balanced', np.unique(train_y), train_y)
  class_weights = dict(enumerate(class_weights))
  # class_weights = {0: 1.0, 1: 15.0}
  print("class weights: ", class_weights)
  print("value counts of Y in train_y: ", train_y.sum())
  print("value counts of N in train_y: ", len(train_y) - train_y.sum())
#+end_src

#+RESULTS:
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1], y=[0 0 0 ... 0 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error
:   warnings.warn(f"Pass {args_msg} as keyword args. From version "
: class weights:  {0: 0.5354749704375247, 1: 7.5472222222222225}
: value counts of Y in train_y:  180
: value counts of N in train_y:  2537

* Grid Search for choosing models

   A few different attempts were made to try to compare some different binary
   classifiers. This is mainly to keep track a backup of notes from the code.
   There were three different binary classifiers used. Logistic Regression
   seems to be the standard starting point for binary classification problems.
   Decisions Trees are flexible, fast, and forgiving so therefore, it was
   tested as a classifier. The last model was a neural network as it was
   hoped to be out compete the other classifiers.

   - [[https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5][Logistic Regression]]
   - [[https://www.upgrad.com/blog/random-forest-vs-decision-tree/][Decision Tree]] and [[https://www.dezyre.com/recipes/optimize-hyper-parameters-of-decisiontree-model-using-grid-search-in-python][another detailed example]]
   - [[https://www.programmersought.com/article/44834680140/][Neural Network Grid Search CV]], and [[https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/][another here]].

   Here are some other resources to look through for selection and comparison
   of different models.

   - [[https://builtin.com/data-science/supervised-machine-learning-classification][AN IN-DEPTH GUIDE TO SUPERVISED MACHINE LEARNING CLASSIFICATION]]
   - [[https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba][What metrics should be used for evaluating a model on an imbalanced data set? (precision + recall or ROC=TPR+FPR)]]
   - [[https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/][How to Choose a Feature Selection Method For Machine Learning]]
   - [[https://www.datasciencecentral.com/profiles/blogs/how-to-choose-a-machine-learning-model-some-guidelines][How to Choose a Machine Learning Model – Some Guidelines]]

   The metric used as the goal for the grid search was the Matthew Correlation
   Coefficient, which considers an equal weighting of false positive and false
   negative rates.

** Overview of results
*** Decision Tree Grid Search

    The following results are from when only using the original 10 features.
    The following pipeline also uses Principal Component Analysis (PCA) first
    and finds the best number of components for the decision tree.

    *PCA Components*:
    - Best number of components: From 1 to the max number of features (10).

    *Decision Tree*:
    - Different Class Weights ~class_weight=[{0: 1.0, 1: w} for w in [10, 12, 14, 15, 16, 17, 18]],~
    - Criterion = ~['gini', 'entropy']~
    - Max Depth = ~[2, 4, 6, 8, 10, 12]~

#+begin_example
 : Best Score: 0.5178250609559948
 : Best Criterion: gini
 : Best max_depth: 12
 : Best Number Of Components: 6
 :
 : DecisionTreeClassifier(class_weight={0: 1.0, 1: 16}, max_depth=12)
#+end_example

*** Logistic Regression Grid Search

     The following results are from when only using the original 10 features.

     - Penalty = ~['l1', 'l2']~
     - C = ~np.logspace(-4, 4, 20),~
     - Different Class Weights ~class_weight=[{0: 1.0, 1: w} for w in [10, 12, 14, 15, 16, 17, 18]],~
     - Solver = ~['liblinear']~

 #+begin_example
Fitting 5 folds for each of 280 candidates, totalling 1400 fits
Best Score: 0.5373897826007131

Best Estimator: LogisticRegression(C=1.623776739188721, class_weight={0: 1.0, 1: 10},
                   penalty='l1', random_state=0, solver='liblinear')
 #+end_example

*** Neural Network Grid Search

**** For the original 10 input features

     The following results are from when only using the original 10 features.
     There were a few iterations since the optimal solutions were endpoints.

     The dataset for this was not balanced! There were no class weights for the
     following result
     - Batch Size = ~[2, 4, 6, 8, 10]~
     - Epochs = ~[100, 125, 150, 175, 200]~

 #+begin_example
 Best Score: 0.6292982089131982
 Best Estimator: {'verbose': 0, 'batch_size': 4, 'epochs': 200, 'build_fn': <function create_model at 0x7f440e43d9d0>}
 #+end_example

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[3, 4, 5]~
     - Epochs = ~[190, 200, 210]~

 #+begin_example
 Best Score: 0.589460143563261

 Best Estimator: {'verbose': 0, 'batch_size': 5, 'epochs': 210, 'build_fn': <function create_model at 0x7f938bf6a9d0>}
 #+end_example

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[4, 6, 10, 20]~
     - Epochs = ~[200, 250, 300, 400]~

 #+begin_example
 Best Score: 0.5824397099036812
 Best Estimator: {'verbose': 0, 'batch_size': 6, 'epochs': 250, 'build_fn': <function create_model at 0x7f938bf5a280>}
 #+end_example

**** For the added 26 input features with 26,13,1 structure

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[5, 6, 7]~
     - Epochs = ~[230, 235, 240, 245, 250]~

 #+begin_example
 Best Score: 0.6141709069236523

 Best Estimator: {'verbose': 0, 'batch_size': 6, 'epochs': 240, 'build_fn': <function create_model at 0x7fe6b47933a0>}
 #+end_example

**** For the added 26 input features with 26,17,9,1 structure

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[2, 4, 6, 8, 10]~
     - Epochs = ~[100, 125, 150, 175, 200]~

 #+begin_example
 Best Score: 0.6292286798742979

 Best Estimator: {'verbose': 0, 'batch_size': 2, 'epochs': 100, 'build_fn': <function create_model at 0x7fe6b4648670>}
 #+end_example

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[1, 2, 4]~
     - Epochs = ~[25, 50, 75, 100, 125]~

 #+begin_example
 Best Score: 0.5979329942840022

 Best Estimator: {'verbose': 0, 'batch_size': 1, 'epochs': 125, 'build_fn': <function create_model at 0x7fe634937040>}
 #+end_example

     The ~class weights~ parameter were set to 'balanced' for this run.
     - Batch Size = ~[1, 2, 4]~
     - Epochs = ~[125, 150, 200, 250, 300]~

 #+begin_example
 Best Score: 0.6079706322981303

 Best Estimator: {'verbose': 0, 'batch_size': 4, 'epochs': 125, 'build_fn': <function create_model at 0x7fe6b47933a0>}
 #+end_example

 It turns out that for this structure, it seems to be peaking at around 0.60 -
 0.63 with:

  - Batch Size: 4
  - Epochs: 125

** Actual Code
*** Using Grid Search for the Decision Tree Classifier

#+begin_src python :session SESSION_1 :results output :exports both :eval no
  pca = decomposition.PCA()
  dec_tree = tree.DecisionTreeClassifier()
  pipe = Pipeline(steps=[('pca', pca),
                         ('dec_tree', dec_tree)])
  n_components = list(range(1,train_X.shape[1]+1,1))
  criterion = ['gini', 'entropy']
  grid_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
  max_depth = [2,4,6,8,10,12]
  parameters = dict(pca__n_components=n_components,
                        dec_tree__criterion=criterion,
                        dec_tree__class_weight=[{0: 1.0, 1: w} for w in [10, 12, 14, 15, 16, 17, 18]],
                        dec_tree__max_depth=max_depth)
  clf_GS = GridSearchCV(pipe, parameters, scoring=grid_scorer)
  clf_GS.fit(train_X, train_y)
  print('Best Score:', clf_GS.best_score_)
  print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])
  print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])
  print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])
  print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])
#+end_src

 #+RESULTS:
 : Best Score: 0.5011699881216227
 : Best Criterion: entropy
 : Best max_depth: 8
 : Best Number Of Components: 22
 :
 : DecisionTreeClassifier(class_weight={0: 1.0, 1: 10}, criterion='entropy',
 :                        max_depth=8)

*** Using Grid Search for the Logistic Regression

#+begin_src python :session SESSION_1 :results output :exports both :eval no
  logistic_regression = LogisticRegression(random_state=0)
  pipe = Pipeline(steps=[('logistic_regression', logistic_regression)])
  grid_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
  parameters = dict(logistic_regression__penalty = ['l1', 'l2'],
                    logistic_regression__C = np.logspace(-4, 4, 20),
                    logistic_regression__class_weight=[{0: 1.0, 1: w} for w in [10, 12, 14, 15, 16, 17, 18]],
                    logistic_regression__solver = ['liblinear'])
  clf_GS = GridSearchCV(pipe, parameters, scoring=grid_scorer, cv = 5, verbose=True, n_jobs=-1, return_train_score=True)
  clf_GS.fit(train_X, train_y)
  print('Best Score:', clf_GS.best_score_)
  print(); print('Best Estimator:', clf_GS.best_estimator_.get_params()['logistic_regression'])
#+end_src

#+RESULTS:
#+begin_example
Fitting 5 folds for each of 280 candidates, totalling 1400 fits
Best Score: 0.6192125559700654

Best Estimator: LogisticRegression(C=29.763514416313132, class_weight={0: 1.0, 1: 10},
                   random_state=0, solver='liblinear')
#+end_example

*** Using Grid Search for Neural Network
    
     Some different model structures were tested for this model, but there were
     some '[[https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw#:~:text=There%20are%20many%20rule-of,size%20of%20the%20output%20layer.][rules of thumb]]' which were considered. For the number of hidden nodes
     in a layer:
     - The number of hidden neurons should be between the size of the input layer
       and the size of the output layer.
     - The number of hidden neurons should be 2/3 the size of the input layer,
       plus the size of the output layer.
     - The number of hidden neurons should be less than twice the size of the
       input layer.

     A summation of these rules were outlined as the following:
     1. number of hidden layers equals one
     2. the number of neurons in that layer is the mean of the neurons in the input and output layers.

#+begin_src python :session SESSION_1 :results output :exports both :eval no
  def create_model():
      model = Sequential([
          Dense(26, activation='relu', input_shape=(26,)),
          Dense(17, activation='relu'),
          Dense(9, activation='relu'),
          Dense(1, activation='sigmoid'),
      ])

      model.compile(
          optimizer= HP['OPTIMIZER'],
          loss= HP['LOSS'],
          metrics=HP['METRICS'])
      return model

  model = KerasClassifier(build_fn=create_model, verbose = 0)

  grid_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
  batch_size = [1, 2, 4]
  epochs = [125, 150, 200, 250, 300]
  param_grid = dict(batch_size=batch_size,
                    epochs=epochs)
  clf_GS = GridSearchCV(model, param_grid, scoring=grid_scorer, cv = 5, n_jobs=-1)
  clf_GS.fit(train_X, train_y, class_weight=class_weights)

  print('Best Score:', clf_GS.best_score_)
  print(); print('Best Estimator:', clf_GS.best_estimator_.get_params())
#+end_src

 #+RESULTS:
 #+begin_example
 Best Score: 0.6079706322981303

 Best Estimator: {'verbose': 0, 'batch_size': 4, 'epochs': 125, 'build_fn': <function create_model at 0x7fe6b47933a0>}
 #+end_example

* The Final Models
** Decision Tree Model
*** Checkpointing Model Weights
     This for setting up the checkpoints while running the model. It is not really
     in use right now as it is not added in the ~model.fit~ callbacks.
  #+begin_src python :session SESSION_1 :results output :exports both
    model_weights_name = HP['NAME'] + '_decision_tree_model.h5'
    checkpointer = ModelCheckpoint(model_weights_name, monitor='Recall', verbose=0)
  #+end_src

  #+RESULTS:

*** Defining and Training the Classifier

  #+begin_src python :session SESSION_1 :results output :exports both
    pca = decomposition.PCA(n_components=22)
    pca.fit(train_X)
    train_X_pca = pca.transform(train_X)
    test_X_pca = pca.transform(test_X)

    decision_tree_model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=8, class_weight={0: 1.0, 1: 10.0})
    decision_tree_model.fit(train_X_pca, train_y)
  #+end_src

  #+RESULTS:

*** Saving or Loading the Classifier
    Both of these blocks can be uncommented to save or load models as desired.
**** Saving the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Saving Entire Model
    # save_model(decision_tree_model, "test_final_check")
    # model.model.save("decision_tree_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

**** Loading the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Loading Entire Model
    # model = load_model(decision_tree_model, "test_final")
    # model.load("decision_tree_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

*** Generating Predictions and Prediction Probabilities 
  The ~model.predict()~ will return the class predictions for the input data put
  in the function. The ~model.predict_proba()~ will return the probability
  predictions for the classes, and is the likelihood of the observation
  belonging to the different classes.

#+begin_src python :session SESSION_1 :results output :exports both
  # Testing the model
  decision_tree_pred = decision_tree_model.predict(test_X_pca)
  decision_tree_y_score = decision_tree_model.predict_proba(test_X_pca)
#+end_src

#+RESULTS:
    
** Logistic Regression Model
*** Checkpointing Model Weights
     This for setting up the checkpoints while running the model. It is not really
     in use right now as it is not added in the ~model.fit~ callbacks.
  #+begin_src python :session SESSION_1 :results output :exports both
    model_weights_name = HP['NAME'] + '_logistic_regression_model.h5'
    checkpointer = ModelCheckpoint(model_weights_name, monitor='Recall', verbose=0)
  #+end_src

  #+RESULTS:

*** Defining and Training the Classifier

#+begin_src python :session SESSION_1 :results output :exports both
  logistic_regression_model = LogisticRegression(C=29.763514416313132, class_weight={0: 1.0, 1: 10},
                                                  random_state=0, solver='liblinear')
  logistic_regression_model.fit(train_X, train_y)
#+end_src

#+RESULTS:

*** Saving or Loading the Classifier
    Both of these blocks can be uncommented to save or load models as desired.
**** Saving the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Saving Entire Model
    # save_model(logistic_regression_model, "test_final_check")
    # model.model.save("logistic_regression_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

**** Loading the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Loading Entire Model
    # model = load_model(logistic_regression_model, "test_final")
    # model.load("logistic_regression_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

*** Generating Predictions and Prediction Probabilities 
  The ~model.predict()~ will return the class predictions for the input data put
  in the function. The ~model.predict_proba()~ will return the probability
  predictions for the classes, and is the likelihood of the observation
  belonging to the different classes.

#+begin_src python :session SESSION_1 :results output :exports both
  # Testing the model
  logistic_regression_pred = logistic_regression_model.predict(test_X)
  logistic_regression_y_score = logistic_regression_model.predict_proba(test_X)
#+end_src

#+RESULTS:
    
** Neural Network Model
*** Checkpointing Model Weights
     This for setting up the checkpoints while running the model. It is not really
     in use right now as it is not added in the ~model.fit~ callbacks.
  #+begin_src python :session SESSION_1 :results output :exports both
    model_weights_name = HP['NAME'] + '_NN_model.h5'
    checkpointer = ModelCheckpoint(model_weights_name, monitor='Recall', verbose=0)
  #+end_src

  #+RESULTS:

*** Defining and Training the Classifier

  #+begin_src python :session SESSION_1 :results output :exports both
    def create_model():
        model = Sequential([
            Dense(26, activation='relu', input_shape=(26,)),
            Dense(13, activation='relu'),
            Dense(1, activation='sigmoid'),
        ])

        model.compile(
            optimizer= HP['OPTIMIZER'],
            loss= HP['LOSS'],
            metrics=HP['METRICS'])
        return model

    nn_model = KerasClassifier(build_fn=create_model, epochs=HP['EPOCHS'],
                                batch_size=HP['BATCH_SIZE'], verbose = 2)
    nn_model.fit(train_X, train_y, class_weight=class_weights, callbacks=[csv_logger, tensorboard_callback])
  #+end_src

  #+RESULTS:
  #+begin_example
  Epoch 1/50
  2021-07-25 00:06:40.488944: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
  2021-07-25 00:06:40.488990: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
  2021-07-25 00:06:40.489041: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
  2021-07-25 00:06:40.515677: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
  2021-07-25 00:06:40.515758: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1496] function cupti_interface_->Finalize()failed with error CUPTI could not be loaded or symbol could not be found.
  2021-07-25 00:06:40.516279: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
  2021-07-25 00:06:40.517294: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
  2021-07-25 00:06:40.518843: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40
  2021-07-25 00:06:40.519653: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.trace.json.gz
  2021-07-25 00:06:40.520995: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40
  2021-07-25 00:06:40.521103: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.memory_profile.json.gz
  2021-07-25 00:06:40.521374: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40Dumped tool data for xplane.pb to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.xplane.pb
  Dumped tool data for overview_page.pb to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.overview_page.pb
  Dumped tool data for input_pipeline.pb to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.input_pipeline.pb
  Dumped tool data for tensorflow_stats.pb to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.tensorflow_stats.pb
  Dumped tool data for kernel_stats.pb to ../logs/scalars/20210725-000632/train/plugins/profile/2021_07_25_00_06_40/BigArch.kernel_stats.pb

  2717/2717 - 2s - loss: 0.5292 - accuracy: 0.8649 - recall: 0.6333
  Epoch 2/50
  2717/2717 - 2s - loss: 0.2411 - accuracy: 0.8756 - recall: 0.9167
  Epoch 3/50
  2717/2717 - 2s - loss: 0.2053 - accuracy: 0.8933 - recall: 0.9167
  Epoch 4/50
  2717/2717 - 2s - loss: 0.1809 - accuracy: 0.9032 - recall: 0.9444
  Epoch 5/50
  2717/2717 - 2s - loss: 0.1745 - accuracy: 0.9087 - recall: 0.9444
  Epoch 6/50
  2717/2717 - 2s - loss: 0.1655 - accuracy: 0.9179 - recall: 0.9667
  Epoch 7/50
  2717/2717 - 2s - loss: 0.1600 - accuracy: 0.9176 - recall: 0.9389
  Epoch 8/50
  2717/2717 - 2s - loss: 0.1611 - accuracy: 0.9157 - recall: 0.9500
  Epoch 9/50
  2717/2717 - 2s - loss: 0.1452 - accuracy: 0.9231 - recall: 0.9722
  Epoch 10/50
  2717/2717 - 2s - loss: 0.1334 - accuracy: 0.9282 - recall: 0.9722
  Epoch 11/50
  2717/2717 - 2s - loss: 0.1396 - accuracy: 0.9245 - recall: 0.9611
  Epoch 12/50
  2717/2717 - 2s - loss: 0.1388 - accuracy: 0.9315 - recall: 0.9778
  Epoch 13/50
  2717/2717 - 2s - loss: 0.1251 - accuracy: 0.9341 - recall: 0.9778
  Epoch 14/50
  2717/2717 - 2s - loss: 0.1326 - accuracy: 0.9360 - recall: 0.9667
  Epoch 15/50
  2717/2717 - 2s - loss: 0.1338 - accuracy: 0.9356 - recall: 0.9611
  Epoch 16/50
  2717/2717 - 2s - loss: 0.1205 - accuracy: 0.9389 - recall: 0.9722
  Epoch 17/50
  2717/2717 - 2s - loss: 0.1133 - accuracy: 0.9418 - recall: 0.9722
  Epoch 18/50
  2717/2717 - 2s - loss: 0.1175 - accuracy: 0.9444 - recall: 0.9611
  Epoch 19/50
  2717/2717 - 2s - loss: 0.1034 - accuracy: 0.9492 - recall: 0.9833
  Epoch 20/50
  2717/2717 - 2s - loss: 0.1097 - accuracy: 0.9437 - recall: 0.9778
  Epoch 21/50
  2717/2717 - 2s - loss: 0.1121 - accuracy: 0.9481 - recall: 0.9667
  Epoch 22/50
  2717/2717 - 2s - loss: 0.1096 - accuracy: 0.9430 - recall: 0.9778
  Epoch 23/50
  2717/2717 - 2s - loss: 0.0990 - accuracy: 0.9496 - recall: 0.9833
  Epoch 24/50
  2717/2717 - 2s - loss: 0.1051 - accuracy: 0.9492 - recall: 0.9889
  Epoch 25/50
  2717/2717 - 2s - loss: 0.0884 - accuracy: 0.9562 - recall: 0.9833
  Epoch 26/50
  2717/2717 - 2s - loss: 0.0970 - accuracy: 0.9510 - recall: 0.9722
  Epoch 27/50
  2717/2717 - 2s - loss: 0.0831 - accuracy: 0.9595 - recall: 0.9889
  Epoch 28/50
  2717/2717 - 2s - loss: 0.0817 - accuracy: 0.9584 - recall: 0.9889
  Epoch 29/50
  2717/2717 - 2s - loss: 0.1072 - accuracy: 0.9488 - recall: 0.9722
  Epoch 30/50
  2717/2717 - 2s - loss: 0.0794 - accuracy: 0.9606 - recall: 0.9944
  Epoch 31/50
  2717/2717 - 2s - loss: 0.0791 - accuracy: 0.9595 - recall: 0.9889
  Epoch 32/50
  2717/2717 - 2s - loss: 0.0805 - accuracy: 0.9610 - recall: 0.9889
  Epoch 33/50
  2717/2717 - 2s - loss: 0.0870 - accuracy: 0.9588 - recall: 0.9722
  Epoch 34/50
  2717/2717 - 2s - loss: 0.0800 - accuracy: 0.9599 - recall: 0.9889
  Epoch 35/50
  2717/2717 - 2s - loss: 0.0799 - accuracy: 0.9588 - recall: 0.9722
  Epoch 36/50
  2717/2717 - 2s - loss: 0.0902 - accuracy: 0.9628 - recall: 0.9833
  Epoch 37/50
  2717/2717 - 2s - loss: 0.1179 - accuracy: 0.9628 - recall: 0.9889
  Epoch 38/50
  2717/2717 - 2s - loss: 0.0657 - accuracy: 0.9672 - recall: 1.0000
  Epoch 39/50
  2717/2717 - 2s - loss: 0.0664 - accuracy: 0.9702 - recall: 0.9833
  Epoch 40/50
  2717/2717 - 2s - loss: 0.0619 - accuracy: 0.9731 - recall: 0.9889
  Epoch 41/50
  2717/2717 - 2s - loss: 0.0609 - accuracy: 0.9687 - recall: 0.9944
  Epoch 42/50
  2717/2717 - 2s - loss: 0.0658 - accuracy: 0.9680 - recall: 0.9889
  Epoch 43/50
  2717/2717 - 2s - loss: 0.0904 - accuracy: 0.9614 - recall: 0.9722
  Epoch 44/50
  2717/2717 - 2s - loss: 0.0754 - accuracy: 0.9628 - recall: 0.9889
  Epoch 45/50
  2717/2717 - 2s - loss: 0.0497 - accuracy: 0.9742 - recall: 0.9944
  Epoch 46/50
  2717/2717 - 2s - loss: 0.0601 - accuracy: 0.9724 - recall: 0.9889
  Epoch 47/50
  2717/2717 - 2s - loss: 0.0524 - accuracy: 0.9739 - recall: 1.0000
  Epoch 48/50
  2717/2717 - 2s - loss: 0.0833 - accuracy: 0.9683 - recall: 0.9833
  Epoch 49/50
  2717/2717 - 2s - loss: 0.1226 - accuracy: 0.9676 - recall: 0.9889
  Epoch 50/50
  2717/2717 - 2s - loss: 0.0567 - accuracy: 0.9724 - recall: 0.9944
  #+end_example

*** Saving or Loading the Classifier
    Both of these blocks can be uncommented to save or load models as desired.
**** Saving the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Saving Entire Model
    # save_model(nn_model, "test_final_check")
    # model.model.save("nn_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

**** Loading the Model
  #+begin_src python :session SESSION_1 :results output :exports both
    ### Loading Entire Model
    # model = load_model(nn_model, "test_final")
    # model.load("nn_model_test_final_different.h5")
  #+end_src

  #+RESULTS:

*** Generating Predictions and Prediction Probabilities 
  The ~model.predict()~ will return the class predictions for the input data put
  in the function. The ~model.predict_proba()~ will return the probability
  predictions for the classes, and is the likelihood of the observation
  belonging to the different classes.

#+begin_src python :session SESSION_1 :results output :exports both
  # Testing the model
  nn_pred = nn_model.predict(test_X)
  nn_y_score = nn_model.predict_proba(test_X, batch_size=HP['BATCH_SIZE'])
  # y_score = model.predict_proba(test_X)
#+end_src

#+RESULTS:
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
:   warnings.warn('`model.predict_classes()` is deprecated and '
: 679/679 - 0s
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.
:   warnings.warn('`model.predict_proba()` is deprecated and '
: 679/679 - 0s
    
* Metrics
   Since the data set is imbalanced, there are other metrics to consider beyond
   the typical accuracy. In this dataset the ratio of HoF players vs. non-HoF
   players is ~14:1~ after preprocessing. Without class weights there could be a
   bias towards never selecting the HoF players resulting an accuracy over 90%
   while always predicting them as non-HoF players.
   
#+begin_src python :session SESSION_1 :results output :exports both
  # Calculating overall metrics
  def print_metrics(test_y, pred, y_score, model_name):
      accuracy = accuracy_score(test_y, pred)
      mcc = matthews_corrcoef(test_y, pred)
      tn, fp, fn, tp = confusion_matrix(test_y, pred).ravel()
      confusion_mat = [tn, fp, fn, tp]
      auroc = roc_auc_score(test_y, y_score[:,0])
      precision = tp/(tp+fp)
      recall = tp/(tp+fn)
      f1 = (2*precision*recall)/(precision+recall)

      # Showing numerical results
      confusion_label = ["tn", "fp", "fn", "tp"]
      for i in range(0,len(confusion_mat)):
          print(confusion_label[i], ': ', confusion_mat[i])
      print("###### ---------Overall Results --------- ######")
      print("model_name: ", model_name)
      print("accuracy: ", accuracy)
      print("MCC: ", mcc)
      print("confusion_mat: ", confusion_mat)
      print("auroc: ", auroc)
      print("precision: ", precision)
      print("recall: ", recall)
      print("f1: ", f1)

      # Saving the metrics
      metric_dict = {
          'Model Name': model_name,
          'True Negative': tn,
          'True Positive': tp,
          'False Negative': fn,
          'False Positive': fp,
          'AUROC': auroc,
          'Accuracy': accuracy,
          'Precision': precision,
          'Recall': recall,
          'F1': f1
      }
      with open("../result/master_log.txt", "a") as file:
          print(metric_dict, file=file)
        
  print_metrics(test_y, decision_tree_pred, decision_tree_y_score, "Decision Tree")
  print_metrics(test_y, logistic_regression_pred, logistic_regression_y_score, "Logistic Regression")
  print_metrics(test_y, nn_pred, nn_y_score, "Neural Network")
#+end_src

#+RESULTS:
#+begin_example
tn :  579
fp :  55
fn :  24
tp :  21
###### ---------Overall Results --------- ######
model_name:  Decision Tree
accuracy:  0.8836524300441826
MCC:  0.29975944655419967
confusion_mat:  [579, 55, 24, 21]
auroc:  0.30552050473186126
precision:  0.27631578947368424
recall:  0.4666666666666667
f1:  0.347107438016529
tn :  572
fp :  62
fn :  8
tp :  37
###### ---------Overall Results --------- ######
model_name:  Logistic Regression
accuracy:  0.8969072164948454
MCC:  0.5106413422190602
confusion_mat:  [572, 62, 8, 37]
auroc:  0.05341745531019977
precision:  0.37373737373737376
recall:  0.8222222222222222
f1:  0.513888888888889
tn :  588
fp :  46
fn :  14
tp :  31
###### ---------Overall Results --------- ######
model_name:  Neural Network
accuracy:  0.9116347569955817
MCC:  0.4835297289342375
confusion_mat:  [588, 46, 14, 31]
auroc:  0.0923764458464774
precision:  0.4025974025974026
recall:  0.6888888888888889
f1:  0.5081967213114754
#+end_example

* TODO Plots
  Three metrics that are smart to include for imbalanced datasets are:
  - ROC curve
  - Confusion Matrix
  - Precision-Recall Matrix

#+begin_src python :session SESSION_1 :results output :exports both
  # ROC curve
  decision_tree_fper, decision_tree_tper, thresholds = roc_curve(test_y, decision_tree_y_score[:,1])
  logistic_regression_fper, logistic_regression_tper, thresholds = roc_curve(test_y, logistic_regression_y_score[:,1])
  nn_fper, nn_tper, thresholds = roc_curve(test_y, nn_y_score[:,1])

  plt.plot(decision_tree_fper, decision_tree_tper, color='orange', label='Decision Tree ROC')
  plt.plot(logistic_regression_fper, logistic_regression_tper, color='red', label='Logistic Regression ROC')
  plt.plot(nn_fper, nn_tper, color='green', label='Neural Network ROC')
  plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver Operating Characteristic (ROC) Curve')
  plt.legend()
  file_name = '../result/ROC_curve_' + HP['NAME'] + datetime.now().strftime("%Y%m%d-%H%M%S") + '.png'
  plt.savefig(file_name)
  plt.savefig("../result/ROC_curve_latest.png")
  plt.clf()

  # plot_roc_curve(fper, tper, HP['NAME'] + '_decision_tree')
  # plot_roc_curve(fper, tper, HP['NAME'] + '_logistic_regression')
  # plot_roc_curve(fper, tper, HP['NAME'] + '_neural_network')

  decision_tree_man_auroc = auc(decision_tree_fper, decision_tree_tper)
  logistic_regression_man_auroc = auc(logistic_regression_fper, logistic_regression_tper)
  nn_man_auroc = auc(nn_fper, nn_tper)
  print("decision_tree_man_auroc: ", decision_tree_man_auroc)
  print("logistic_regression_man_auroc: ", logistic_regression_man_auroc)
  print("nn_man_auroc: ", nn_man_auroc)
#+end_src

#+RESULTS:
: decision_tree_man_auroc:  0.6944794952681388
: logistic_regression_man_auroc:  0.9465825446898002
: nn_man_auroc:  0.920837714686295

The latest ROC_plot is the following plot.

[[file:../result/ROC_curve_latest.png]]


#+begin_src python :session SESSION_1 :results output :exports both
  # Generating a confusion matrix
  def create_confusion_matrix(test_y, pred, model_name):
      skplt.metrics.plot_confusion_matrix(test_y, pred, normalize=True)
      confusion_mat_string = "../result/confusion_mat_" + HP['NAME'] + model_name + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
      plt.savefig(confusion_mat_string)
      latest_model_name = "../result/confusion_mat_" + model_name + "_latest.png"
      plt.savefig(latest_model_name)
      plt.clf()

  create_confusion_matrix(test_y, decision_tree_pred, "decision_tree")
  create_confusion_matrix(test_y, logistic_regression_pred, "logistic_regression")
  create_confusion_matrix(test_y, nn_pred, "nn")
#+end_src

#+RESULTS:
: /home/olav/gitRepos/TTK28-project/venv/lib/python3.8/site-packages/scikitplot/metrics.py:115: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
:   fig, ax = plt.subplots(1, 1, figsize=figsize)


 This is the confusion matrix for the Decision Tree Classifier
[[file:../result/confusion_mat_decision_tree_latest.png]]

 This is the confusion matrix for the Logistic Regression Classifier
[[file:../result/confusion_mat_logistic_regression_latest.png]]

 This is the confusion matrix for the Neural Network Model
[[file:../result/confusion_mat_nn_latest.png]]


#+begin_src python :session SESSION_1 :results output :exports both
# Generating precision-recall curve
skplt.metrics.plot_precision_recall(test_y, decision_tree_y_score)
precision_recall_curve_string = "../result/precision_recall_curve_" + HP['NAME'] + "_decision_tree_" + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(precision_recall_curve_string)
plt.savefig("../result/precision_recall_curve_decision_tree_latest.png")
plt.clf()
skplt.metrics.plot_precision_recall(test_y, logistic_regression_y_score)
precision_recall_curve_string = "../result/precision_recall_curve_" + HP['NAME'] + "_logistic_regression_" + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig(precision_recall_curve_string)
plt.savefig("../result/precision_recall_curve_logistic_regression_latest.png")
plt.clf()
skplt.metrics.plot_precision_recall(test_y, nn_y_score)
precision_recall_curve_string = "../result/precision_recall_curve_" + HP['NAME'] + "_nn_" + datetime.now().strftime("%Y%m%d-%H%M%S") + ".png"
plt.savefig("../result/precision_recall_curve_nn_latest.png")
plt.savefig(precision_recall_curve_string)
plt.clf()
#+end_src

#+RESULTS:


 This is the Precision Recall Curve for the Decision Tree Classifier
[[file:../result/precision_recall_curve_decision_tree_latest.png]]

 This is the Precision Recall Curve for the Logistic Regression
[[file:../result/precision_recall_curve_logistic_regression_latest.png]]

 This is the Precision Recall Curve for the Neural Network
[[file:../result/precision_recall_curve_nn_latest.png]]


* Links to Other Files in Project

  1. [[https://www.olavpedersen.com/standalone_hof/extract_data.html][extract_data]]: Extracting data from Lahman's raw data.
  2. [[https://www.olavpedersen.com/standalone_hof/filter_data.html][filter_data]]: Data manipulation, feature creation and feature selection.
  3. [[https://www.olavpedersen.com/standalone_hof/hall_of_fame_model.html][hof_model]]: Creation of the model and training of the neural network.

* Local Variables                                          :noexport:ARCHIVE:
# Local Variables:
# after-save-hook: org-html-export-to-html
# End:
