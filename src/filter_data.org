#+TITLE: Filtering the Data from the Aggregated Player Data
#+STARTUP: headlines
#+STARTUP: nohideblocks
#+STARTUP: noindent
#+OPTIONS: toc:4 h:4 ^:nil _:nil
#+PROPERTY: header-args:emacs-lisp :comments link
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
#+EXPORT_FILE_NAME: filter_data.html
#+HTML_HEAD: <style> #content{max-width:1800px;}</style>

* Activating Python Environment

  This code block must be run to activate python virtual environment for
  org session "SESSION_1". The following Python code blocks are run in
  "SESSION_1" in which the virtual environment should have been activated.

#+BEGIN_SRC emacs-lisp :session SESSION_1 :exports both
  (pyvenv-activate "~/gitRepos/TTK28-project/venv/")
#+END_SRC

#+RESULTS:

* Importing dependencies
  This filtering of data typically uses ~pandas~ and ~numpy~ for data storage
  and manipulation. Additionally a the Random function is used for picking out
  the data. The data is standardized using ~Sklearn~ and the ~StandardScalar~.

#+begin_src python :session SESSION_1 :results output :exports both
import pandas as pd
import numpy as np
from numpy.random import RandomState
from icecream import ic
from sklearn.preprocessing import StandardScaler, OneHotEncoder
#+end_src

#+RESULTS:

* Hyperparameters
  In order to more easily configure the model and keep track of the setting,
  this hyperparameters dictionary is defined here and used throughout the code
  where needed.

#+begin_src python :session SESSION_1 :results output :exports both
NAME = "full"
DATA_BALANCE_PERCENTAGE = 0.8
TRAINING_DATA_FRACTION = 0.8
#+end_src

#+RESULTS:

* Helper Function

   This function determines the number of samples to extract based on a
   percentage how imbalanced the dataset should be.

#+begin_src python :session SESSION_1 :results output :exports both
def num_sample(percentage, hof_df):
    hof_length = len(hof_df['G_all'])
    return int((percentage * hof_length)/(1 - percentage))
#+end_src

#+RESULTS:

* Importing and Cleaning the Data

#+begin_src python :session SESSION_1 :results output :exports both
df = pd.read_csv('../data/final_data.csv')
#+end_src

#+RESULTS:
: sys:1: DtypeWarning: Columns (48) have mixed types.Specify dtype option on import or set low_memory=False.

  Fixing Last Game Played date into a ~float32~.

#+begin_src python :session SESSION_1 :results output :exports both
# Splits the final Game date and convert to string
finalGameYear = df['finalGame'].str.split(pat="-", expand=True)[0]
df['finalGame'] = finalGameYear
# Removing NaN fields from the dataset
df = df[finalGameYear.notnull()]
# Adding back to dataset as ints
df['finalGame'] = df['finalGame'].astype('float32')
#+end_src

#+RESULTS:

  Fixing parts that cannot have NaN.

#+begin_src python :session SESSION_1 :results output :exports both
  # Replacing NaN in each column with 0 and adding back as type
  award_names = ['Most Valuable Player', 'World Series MVP', 'AS_games', 'Gold Glove',
                 'Rookie of the Year', 'Silver Slugger','G', 'AB', 'R', 'H', '2B', '3B',
                 'HR', 'RBI', 'SB', 'BB', 'SO', 'IBB', 'HBP', 'SH', 'SF', 'BB-A', 'H-A',
                 'IPouts', 'SO-A', 'BB-A', 'IBB-A', 'PO', 'A', 'E', 'DP', 'ERA']
  for col in award_names:
      df[col] = df[col].fillna(0)
      df[col] = df[col].astype('float32')
#+end_src

#+RESULTS:

* Creating Additional Statistics

  In order to aggregate the data into stats which are considered features/stats
  which express the skill of a batter. It would be possible to use all the
  features individually, but with a smaller and imbalanced dataset it would be
  nice not to have to include more features than necessary. These stats are 1B,
  SLG, OBS, OPS.

#+begin_src python :session SESSION_1 :results output :exports both
  # Adding the Batting Stats
  df['1B'] = df['H'] - df['2B'] - df['3B'] - df['HR']
  df['SLG'] = (df['1B'] + 2*df['2B'] + 3*df['3B'] + 4*df['HR'])/df['AB']
  df['OBS'] = (df['H'] + df['BB'] + df['HBP'])/(df['AB'] + df['BB']+ df['HBP']+ df['SF'])
  df['OPS'] = df['SLG'] + df['OBS']

  # Adding the Pitching Stats
  df['WHIP'] = round(3*(df['BB-A'] + df['H-A'])/df['IPouts'], 2)
  df['KperBB'] = round(df['SO-A']/(df['BB-A'] - df['IBB-A']), 2)

  new_stats = ['1B', 'SLG', 'OBS', 'OPS', 'WHIP', 'KperBB']
  for col in new_stats:
      df[col] = df[col].fillna(0)
      df[col] = df[col].astype('float32')
#+end_src

#+RESULTS:

* Fulfilling criteria for HoF eligibility

  Removing players who have played less than 10 years as these players are not
  eligible for the MLB Hall of Fame.

#+begin_src python :session SESSION_1 :results output :exports both
  df = df[df['Years_Played'] >= 10] # 10 years
#+end_src

#+RESULTS:

* Preparing to Feed Data into Model

** Selecting Variables to use in training

#+begin_src python :session SESSION_1 :results output :exports both
   hof_label_df = df[['playerID', 'HoF', 'POS']]
   df_inf_containing = df[['WHIP', 'KperBB']]
   df = df[['G_all', 'finalGame', 'OPS', 'SB', 'HR',
            'Years_Played', 'Most Valuable Player', 'AS_games',
            'Gold Glove', 'Rookie of the Year', 'World Series MVP', 'Silver Slugger',
            'WHIP', 'ERA', 'KperBB', 'PO', 'A', 'E', 'DP']]

   df_inf = np.isinf(df_inf_containing)
   # Removing index of values with infinity values from both data to be
   # standardized and the label data
   df = df[~df_inf['WHIP']]
   df = df[~df_inf['KperBB']]
   hof_label_df = hof_label_df[~df_inf['WHIP']]
   hof_label_df = hof_label_df[~df_inf['KperBB']]
#+end_src

#+RESULTS:
: /tmp/babel-aDvIBM/python-PmDC6f:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
:   df = df[~df_inf['KperBB']]
: /tmp/babel-aDvIBM/python-PmDC6f:14: UserWarning: Boolean Series key will be reindexed to match DataFrame index.
:   hof_label_df = hof_label_df[~df_inf['KperBB']]

** One Hot Encoding for Player Position Data

#+begin_src python :session SESSION_1 :results output :exports both
  # creating one hot encoder object 
  onehotencoder = OneHotEncoder()
  #reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object 
  X = onehotencoder.fit_transform(hof_label_df[['POS']]).toarray()
  #To add this back into the original dataframe 
  dfOneHot = pd.DataFrame(X, columns = ["POS_"+str(int(i)) for i in range(X.shape[1])]) 
#+end_src

 #+RESULTS:

** Standardizing the Data

#+begin_src python :session SESSION_1 :results output :exports both
  df = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns)
  # Joining the one hot encoded dummy variables to standardized data set
  df = df.join(dfOneHot)
  # Adding back the HoF data
  df.insert(df.shape[1], 'HoF', hof_label_df['HoF'].to_numpy())
#+end_src

 #+RESULTS:

** Converting HoF back to strings

#+begin_src python :session SESSION_1 :results output :exports both
 df['HoF'] = df['HoF'].replace(1.0, 'Y', regex=True)
 df['HoF'] = df['HoF'].replace(np.nan, 'N', regex=True)
 df['HoF'] = df['HoF'].replace(0.0, 'N', regex=True)
#+end_src

 #+RESULTS:

** Splitting dataset by HoF players

 #+begin_src python :session SESSION_1 :results output :exports both
reg_df = df[df['HoF'] == 'N']
hof_df = df[df['HoF'] == 'Y']
 #+end_src

 #+RESULTS:

** Under-sampling the non-HoF players

   Currently, the non-HoF players are not undersampled.

 #+begin_src python :session SESSION_1 :results output :exports both
reg_seeded_random = RandomState(1)
# sampled_reg_df = reg_df.sample(n = num_sample(DATA_BALANCE_PERCENTAGE, hof_df), random_state=reg_seeded_random)
sampled_reg_df = reg_df
 #+end_src

 #+RESULTS:

** Splitting reg and HoF into train and test

 #+begin_src python :session SESSION_1 :results output :exports both
sep_seeded_random = RandomState(1)
train_reg_df = sampled_reg_df.sample(frac=TRAINING_DATA_FRACTION, random_state=sep_seeded_random)
test_reg_df = sampled_reg_df.drop(train_reg_df.index)
train_hof_df = hof_df.sample(frac=TRAINING_DATA_FRACTION, random_state=sep_seeded_random)
test_hof_df = hof_df.drop(train_hof_df.index)

print('length of train_reg_df: ', len(train_reg_df))
print('length of test_reg_df: ', len(test_reg_df))
print('length of train_hof_df: ', len(train_hof_df))
print('length of test_hof_df: ', len(test_hof_df))
 #+end_src

 #+RESULTS:
 : length of train_reg_df:  2537
 : length of test_reg_df:  634
 : length of train_hof_df:  180
 : length of test_hof_df:  45

** Merging the test and training datasets

 #+begin_src python :session SESSION_1 :results output :exports both
train_df = pd.concat([train_hof_df, train_reg_df])
test_df = pd.concat([test_hof_df, test_reg_df])
# Shuffling the data
train_df = train_df.sample(frac = 1, random_state=sep_seeded_random)
test_df = test_df.sample(frac = 1, random_state=sep_seeded_random)
 #+end_src

 #+RESULTS:

* Save the different testing and validation data sets

 #+begin_src python :session SESSION_1 :results output :exports both
train_df.to_csv('../data/train_data_' + NAME + '.csv', index=False)
test_df.to_csv('../data/test_data_' + NAME + '.csv', index=False)
 #+end_src

 #+RESULTS:

* Links to Other Files in Project

  1. [[https://www.olavpedersen.com/standalone_hof/extract_data.html][extract_data]]: Extracting data from Lahman's raw data.
  2. [[https://www.olavpedersen.com/standalone_hof/filter_data.html][filter_data]]: Data manipulation, feature creation and feature selection.
  3. [[https://www.olavpedersen.com/standalone_hof/hall_of_fame_model.html][hof_model]]: Creation of the model and training of the neural network.


* Local Variables                                          :noexport:ARCHIVE:
# Local Variables:
# after-save-hook: org-html-export-to-html
# End:
